{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UE mobility prediction\n",
    "\n",
    "The UE connects via a DN to the closest application server. Next, it moves to the next edge site. To keep latency low after a gNB handover, the UEâ€™s context should be relocated not only on the gNB, but also on the UPF and the application server.\n",
    "\n",
    "If the network knows in advance that the UE will move to the target edge site, then some parts of the relocation procedure can be done before the actual gNB handover. In this way, the overall procedure is shortened, and the handover can be performed more smoothly.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"../image.png\" width=\"400\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "The dataset in `trajectories.json.gz` is made of events carrying the following features:\n",
    "* `currentEnb`: gNB the user has moved to \n",
    "* `eventId`: type of event\n",
    "* `imsi`: IMSI (user identifier) the event belongs to. \n",
    "* `timestamp`: minutes starting from 0 at day 1 in the dataset. The dataset contains data from about four days and a half \n",
    "* `timeSlot`: a 15-minute slot generated from timestamp. There should be 96 timeslots in a day. `timeSlot` is restarted every day. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('trajectories.json.gz', compression='gzip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['imsi'] == 2785554734]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['currentEnb'].value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timeSlot'].value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'].hist(bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each gNodeB, `topology_neighbours.json` contains its closest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_neigh = pd.read_json('topology_neighbours.json', orient='index')\n",
    "gnb_neigh.reset_index(inplace=True)\n",
    "gnb_neigh.columns = ['gnodeB', 'neigh_1', 'neigh_2', 'neigh_3', 'neigh_4']\n",
    "gnb_neigh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "First, we select the first day as train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_1 = df.loc[(df['timestamp']>=0) & (df['timestamp']<1440)]\n",
    "day_2 = df.loc[(df['timestamp']>=1440) & (df['timestamp']<2880)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each EMB event, we create a number of new features:\n",
    "* `stay_time`: time until the same user moves to another gNodeB (and thus a new event is received).\n",
    "* `transit_time`: time it took the user to move from the the previous gNodeB. For a same user that goes from gNodeB-a to gNodeB-b, `stay_time` in gNodeB-A is the same as `transit_time` in gNodeB-b\n",
    "* `transition_slot`: bucketed `transit_time` considering that any time greater than 60 minutes is assigned 100 minutes.\n",
    "* `time_of_day`: bucketed `timeSlot` (five periods of time in the day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = day_1.set_index(['imsi', 'timestamp']).sort_index().reset_index()\n",
    "grouped_df['next_timestamp'] = grouped_df.groupby(['imsi'])['timestamp'].shift(-1)\n",
    "grouped_df['prev_timestamp'] = grouped_df.groupby(['imsi'])['timestamp'].shift(1)\n",
    "grouped_df['stay_time'] = grouped_df['next_timestamp'] - grouped_df['timestamp']\n",
    "grouped_df['transit_time'] = grouped_df['timestamp'] - grouped_df['prev_timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['stay_time'].hist(bins=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the peak around 700 minutes, that could correspond to people remaining at work for such period of time. It's interesting to note than bucketing to create `transition_slot` takes the maximum value of its bucket but in the larger bucket, where the value is set to 100. That is, all values greater than 60 (one hour) are set to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['transit_time'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_transit_time = grouped_df['transit_time'].max()\n",
    "grouped_df['transition_slot'] = pd.cut(grouped_df['transit_time'], \n",
    "                                       [0, 1, 5, 15, 30, 60, max_transit_time],\n",
    "                                       labels=[1, 5, 15, 30, 60, 100])\n",
    "grouped_df['transition_slot'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['transition_slot'].value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five equal bins are defined. However, as the upper limit is the maximum value in `timeSlot`, assignment has not much to do with the intended meaning of the bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['time_of_day'] = pd.cut(grouped_df['timeSlot'],\n",
    "                                   bins=5,\n",
    "                                   labels=['early_morn', 'morning', 'noon', 'evening', 'night'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['time_of_day'].value_counts().reindex(index=['early_morn', 'morning', 'noon', 'evening', 'night'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['time_of_day'].value_counts().reindex(index=['early_morn', 'morning', 'noon', 'evening', 'night']).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ue_home_df` records how long a given user is at a given gNodeB (from `stay_time`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ue_home_df = grouped_df.groupby(['imsi', 'currentEnb'])['stay_time'].sum().reset_index()\n",
    "ue_home_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`home_df` records the gNodeB where each user stays for longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_gnb = ue_home_df.loc[ue_home_df.groupby(['imsi'])['stay_time'].idxmax()][['imsi', 'currentEnb']]\n",
    "home_gnb.columns = ['imsi', 'home_gnb']\n",
    "home_gnb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ue_df` records the number of gNodeB's a given user is in for each time of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ue_df = grouped_df.groupby(['imsi', 'time_of_day'])['currentEnb'].count().reset_index()\n",
    "ue_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ue_ctxt_df` contains the same information than `ue_df` but pivotted to show a user per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ue_ctxt_df = pd.pivot_table(ue_df, index='imsi', columns='time_of_day', values='currentEnb', fill_value=0)\n",
    "ue_ctxt_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`seq_df` is an auxiliary dataframe that, for each user, provides its trajectory, the time where each transition happened, the bucketed time it took for each user to move from the previous gNodeB and the signal strength in each gNodeB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_df = grouped_df.groupby(['imsi'])['currentEnb', 'timeSlot','transition_slot'] \\\n",
    "                   .agg(lambda x: list(x)).reset_index()\n",
    "\n",
    "seq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_sequence(seq, window_size=5):\n",
    "    \"\"\"This function turns a sequence intro \n",
    "    a matrix where each row is a sequence of\n",
    "    the previous 'windows_size' elements\"\"\"\n",
    "    \n",
    "    windows = []\n",
    "    seq_len = len(seq)\n",
    "    for i in range(len(seq) - window_size + 1):\n",
    "        window = []\n",
    "        for j in range(i, i + window_size):\n",
    "            window.append(seq[j])\n",
    "        windows.append(window)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each feature in `seq_df` creates a new feature where the sequences from the original feature is recorded (five elements in each sequence):\n",
    "* `gnode_seq` records the sequences of each gNodeB in `currentEnb`.\n",
    "* `time_seq` records the sequences of each timeslots in `timeSlot`.\n",
    "* `trans_seq` records the sequences of each transition slots in `transition_slot`.\n",
    "\n",
    "Additionally, we create a new feature with the number of transitions:\n",
    "* `seq_len`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_df['gnode_seq'] = seq_df['currentEnb'].apply(window_sequence).values\n",
    "seq_df['time_seq'] = seq_df['timeSlot'].apply(window_sequence).values\n",
    "seq_df['trans_seq'] = seq_df['transition_slot'].apply(window_sequence).values\n",
    "seq_df['seq_len'] = seq_df['gnode_seq'].apply(lambda x: len(x))\n",
    "seq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we transform each sequence feature into a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enode_seq_list = [sequence for sequences in seq_df['gnode_seq'] for sequence in sequences]\n",
    "time_seq_list = [sequence for sequences in seq_df['time_seq'] for sequence in sequences]\n",
    "trans_seq_list = [sequence for sequences in seq_df['trans_seq'] for sequence in sequences]\n",
    "\n",
    "enode_df = pd.DataFrame(enode_seq_list, columns=['gnode_1', 'gnode_2', 'gnode_3', 'gnode_4', 'target_gnb'])\n",
    "time_df = pd.DataFrame(time_seq_list, columns=['time_1', 'time_2', 'time_3', 'time_4', 'target_time'])\n",
    "trans_df = pd.DataFrame(trans_seq_list, columns=['trans_1', 'trans_2', 'trans_3', 'trans_4', 'target_trans_slot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsi_list = list()\n",
    "for a, b in zip(seq_df['imsi'], seq_df['seq_len']):\n",
    "    imsi_list.extend([a] * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the dataset that will be used for prediction. What we want to predict is what follows: given a detected transition to a given gNodeB, which will be the following gNodeB and when will it happen?\n",
    "\n",
    "Thus, the targets are:\n",
    "* `target_gnb`: Next gNodeB\n",
    "* `target_trans_slot`: Time it will take to move to the next gNodeB.\n",
    "\n",
    "The following features will be considered:\n",
    "* `enode_1`: The fourth previous gNodeB in a given sequence. Depends on the current user's trajectory.\n",
    "* `enode_2`: The third previous gNodeB in a given sequence. Depends on the current user's trajectory.\n",
    "* `enode_3`: The second previous gNodeB in a given sequence. Depends on the current user's trajectory.\n",
    "* `enode_4`: The first previous gNodeB in a given sequence. Depends on the current user's trajectory.\n",
    "* `time_1`: The time where the user moved to the fourth previous gNodeB in a given sequence. Depends on the current user's trajectory.\n",
    "* `time_2`: The time where the user moved to the third previous gNodeB in a given sequence. Depends on the current user's trajectory.\n",
    "* `time_3`: The time where the user moved to the second previous gNodeB in a given sequence. Depends on the current user's trajectory.\n",
    "* `time_4`: The time where the user moved to the first previous gNodeB in a given sequence. Depends on the current user's trajectory.\n",
    "* `home_gnb`: The gNodeB where the user doing for a given trajectory stays more times within a day. It's is a historical data.\n",
    "* `early_morn`: The number of gNodeB's where the user for a given trajectory stays in the early morning. It's is a historical data.\n",
    "* `morning`: The number of gNodeB's where the user for a given trajectory stays in the morning. It's is a historical data.\n",
    "* `noon`: The number of gNodeB's where the user for a given trajectory stays at noon. It's is a historical data.\n",
    "* `evening`: The number of gNodeB's where the user for a given trajectory stays in the evening. It's is a historical data.\n",
    "* `night`: The number of gNodeB's where the user for a given trajectory stays at night. It's is a historical data.\n",
    "* `neigh_1`: One of the four closest gNodeB's to `enode_4`. It's is a static data.\n",
    "* `neigh_2`: One of the four closest gNodeB's to `enode_4`. It's is a static data.\n",
    "* `neigh_3`: One of the four closest gNodeB's to `enode_4`. It's is a static data.\n",
    "* `neigh_4`: One of the four closest gNodeB's to `enode_4`. It's is a static data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CHANGES\n",
    "X_train = pd.concat([enode_df, time_df], axis=1)\n",
    "X_train['target_trans_slot'] = trans_df['target_trans_slot']\n",
    "X_train['imsi'] = imsi_list\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.merge(home_gnb, on='imsi').merge(ue_ctxt_df, on='imsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.merge(gnb_neigh, left_on='gnode_4', right_on='gnodeB', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train['gnodeB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dropna(inplace=True)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train[['target_gnb', 'target_trans_slot']]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop_cols = ['target_gnb',\n",
    "                'target_trans_slot',\n",
    "                'target_time',\n",
    "                'imsi'\n",
    "               ]\n",
    "X_train.drop(columns=to_drop_cols, axis=1, inplace=True)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=30,\n",
    "                               min_samples_leaf=2,\n",
    "                               min_samples_split=2,\n",
    "                               max_depth=15,\n",
    "                               oob_score=True,\n",
    "                               random_state=44)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = model.feature_importances_\n",
    "for i, v in zip(X_train.columns, importance):\n",
    "    print(f'Feature: {i}, Score: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(index=X_train.columns, data=importance).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_accuracy(y, y_pred):\n",
    "    y_pred = pd.DataFrame(y_pred, columns=['target_gnb', 'target_trans_slot'])\n",
    "    gnb_pred_acc = (sum(y.target_gnb.values == y_pred.target_gnb.values) / len(y)) * 100\n",
    "    timeslot_pred_acc = (sum(y.target_trans_slot.values == y_pred.target_trans_slot.values) / len(y)) * 100\n",
    "    pred_accuracy = (sum((y.target_gnb.values == y_pred.target_gnb.values) &\n",
    "                            (y.target_trans_slot.values == y_pred.target_trans_slot.values)) / len(y)) * 100\n",
    "    return {\n",
    "        \"predictionAccuracy\": pred_accuracy,\n",
    "        \"predictionTimeSlotAccuracy\": timeslot_pred_acc,\n",
    "        \"predictionGnodeBAccuracy\": gnb_pred_acc\n",
    "    }\n",
    "\n",
    "def topk_accuracy(actual_labels, pred_gnb, pred_time, k=3):\n",
    "    actual_labels_size = len(actual_labels['target_gnb'])\n",
    "    predictions_size = len(pred_gnb)\n",
    "    if actual_labels_size != predictions_size:\n",
    "        raise ValueError(\"actual and predicted should be of same size\")\n",
    "    results = np.zeros(actual_labels_size)\n",
    "    for i in range(actual_labels_size):\n",
    "        if ((actual_labels['target_gnb'][i] in set(pred_gnb[i][:k])) & (actual_labels['target_trans_slot'][i] in set(pred_time[i][:k]))):\n",
    "            results[i] = 1\n",
    "    result = results.sum()/actual_labels_size\n",
    "    print(f\"predictionAccuracy with first {k} results: {result}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = compute_accuracy(y_train, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_preds = model.classes_[0][np.argsort(-probs[0])]\n",
    "ts_preds = model.classes_[1][np.argsort(-probs[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_accuracy(y_train.reset_index(), gnb_preds, ts_preds, k=1)\n",
    "topk_accuracy(y_train.reset_index(), gnb_preds, ts_preds, k=3)\n",
    "topk_accuracy(y_train.reset_index(), gnb_preds, ts_preds, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class implements the very same data processing procedure used above. It's been compacted for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessData:\n",
    "    def __init__(self, training_df,gnb_neigh):\n",
    "        self.training_df = training_df\n",
    "        self.gnb_neigh = gnb_neigh\n",
    "    \n",
    "    @staticmethod\n",
    "    def window_sequence(seq, window_size=5):\n",
    "        windows = []\n",
    "        seq_len = len(seq)\n",
    "        if seq_len < window_size:\n",
    "            print(seq)\n",
    "            padding_len = window_size - seq_len\n",
    "        for i in range(len(seq) - window_size + 1):\n",
    "            window = []\n",
    "            for j in range(i, i + window_size):\n",
    "                window.append(seq[j])\n",
    "            windows.append(window)\n",
    "        return windows\n",
    "\n",
    "    def compute_home_gnb(self, grouped_df):\n",
    "        ue_home_df = grouped_df.groupby(['imsi', 'currentEnb'])['stay_time'].sum().reset_index()\n",
    "        home_gnb = ue_home_df.loc[ue_home_df.groupby(['imsi'])['stay_time'].idxmax()][['imsi', 'currentEnb']]\n",
    "        home_gnb.columns = ['imsi', 'home_gnb']\n",
    "        return home_gnb\n",
    "\n",
    "    def compute_ue_context_df(self,grouped_df):\n",
    "        grouped_df['time_of_day'] = pd.cut(grouped_df['timeSlot'], \n",
    "                                           bins=5,\n",
    "                                           labels=['early_morn', 'morning', 'noon', 'evening', 'night'])\n",
    "        ue_df = grouped_df.groupby(['imsi', 'time_of_day'])['currentEnb'].count().reset_index()\n",
    "        ue_ctxt_df = pd.pivot_table(ue_df, index='imsi', columns='time_of_day', values='currentEnb', fill_value=0)\n",
    "        return ue_ctxt_df\n",
    "\n",
    "    def compute_transition_df(self, grouped_df):\n",
    "        seq_df = grouped_df.groupby(['imsi'])['currentEnb', 'timeSlot','transition_slot'].agg(\n",
    "            lambda x: list(x)).reset_index()\n",
    "        seq_df['enode_seq'] = seq_df['currentEnb'].apply(self.window_sequence).values\n",
    "        seq_df['time_seq'] = seq_df['timeSlot'].apply(self.window_sequence).values\n",
    "        seq_df['trans_seq'] = seq_df['transition_slot'].apply(self.window_sequence).values\n",
    "        enode_seq_list = [sequence for sequences in seq_df['enode_seq'] for sequence in sequences]\n",
    "        time_seq_list = [sequence for sequences in seq_df['time_seq'] for sequence in sequences]\n",
    "        trans_seq_list = [sequence for sequences in seq_df['trans_seq'] for sequence in sequences]\n",
    "        seq_df['seq_len'] = seq_df['enode_seq'].apply(lambda x: len(x))\n",
    "        imsi_list = list()\n",
    "        for a, b in zip(seq_df['imsi'], seq_df['seq_len']):\n",
    "            imsi_list.extend([a] * b)\n",
    "        enode_df = pd.DataFrame(enode_seq_list, columns=['enode_1', 'enode_2', 'enode_3', 'enode_4', 'target_gnb'])\n",
    "        time_df = pd.DataFrame(time_seq_list, columns=['time_1', 'time_2', 'time_3', 'time_4', 'target_time'])\n",
    "        trans_df = pd.DataFrame(trans_seq_list, columns=['trans_1', 'trans_2', 'trans_3', 'trans_4', 'target_trans_slot'])\n",
    "        df = pd.concat([enode_df, time_df], axis=1)\n",
    "        #df = pd.concat([enode_df, time_df], axis=1)\n",
    "        df['target_trans_slot'] = trans_df['target_trans_slot']\n",
    "        df['imsi'] = imsi_list\n",
    "        return df\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        grouped_df = self.training_df.set_index(['imsi', 'timestamp']).sort_index().reset_index()\n",
    "        grouped_df['next_timstamp'] = grouped_df.groupby(['imsi'])['timestamp'].shift(-1)\n",
    "        grouped_df['prev_timstamp'] = grouped_df.groupby(['imsi'])['timestamp'].shift(1)\n",
    "        grouped_df['stay_time'] = grouped_df['next_timstamp'] - grouped_df['timestamp']\n",
    "        grouped_df['transit_time'] = grouped_df['timestamp'] - grouped_df['prev_timstamp']\n",
    "        grouped_df['transition_slot'] = pd.cut(grouped_df['transit_time'], [0, 1, 5, 15, 30, 60, 3000],\n",
    "                                               labels=[1, 5, 15, 30, 60, 100])\n",
    "\n",
    "        home_gnb = self.compute_home_gnb(grouped_df)\n",
    "        ue_ctxt_df = self.compute_ue_context_df(grouped_df)\n",
    "\n",
    "        df = self.compute_transition_df(grouped_df)\n",
    "\n",
    "        df = df.merge(home_gnb, on='imsi').merge(ue_ctxt_df, on='imsi')\n",
    "        df = df.merge(self.gnb_neigh, left_on='enode_4', right_on='gnodeB', how='left')\n",
    "        del df['gnodeB']\n",
    "        df.dropna(inplace=True)\n",
    "        y_train = df[['target_gnb', 'target_trans_slot']]\n",
    "\n",
    "        to_drop_cols = ['target_gnb',\n",
    "                        'target_trans_slot',\n",
    "                        'target_time',\n",
    "                        'imsi'\n",
    "                       ]\n",
    "        print(df.shape)\n",
    "        df.drop(columns=to_drop_cols, inplace=True, axis=1)\n",
    "        \n",
    "        return df, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_2 = df.loc[(df['timestamp']>=1440) & (df['timestamp']<2880)]\n",
    "\n",
    "preprocess_df = PreprocessData(day_2,gnb_neigh)\n",
    "X_test, y_test = preprocess_df.preprocess_data()\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict_proba(X_test)\n",
    "gnb_preds = model.classes_[0][np.argsort(-probs[0])]\n",
    "ts_preds = model.classes_[1][np.argsort(-probs[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_accuracy(y_test.reset_index(), gnb_preds, ts_preds, k=1)\n",
    "topk_accuracy(y_test.reset_index(), gnb_preds, ts_preds, k=3)\n",
    "topk_accuracy(y_test.reset_index(), gnb_preds, ts_preds, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = compute_accuracy(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model sharing\n",
    "### Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'mobility_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX\n",
    "\n",
    "Open Neural Network Exchange (ONNX) is an open standard format for AI models, born initially with focus on deep learning, but eventually extended to traditional ML. It defines an extensible computation graph model, as well as definitions of built-in operators and standard data types so that it is possible to represent any arbitrary model.\n",
    "\n",
    "It is possible for most of the popular ML frameworks to export trained models as ONNX models. For scikit-learn: [sklearn-onnx](http://onnx.ai/sklearn-onnx/). The converted ONNX model can be run by means of the [ONNX Runtime](https://www.onnxruntime.ai/) (thus enabling the creation of microservices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import Int64TensorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_onnx = X_train.to_numpy()\n",
    "X_test_onnx = X_test.to_numpy()\n",
    "y_train_onnx = y_train.to_numpy()\n",
    "y_test_onnx = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_type = [('input', Int64TensorType([None, 18]))]\n",
    "onx = convert_sklearn(model, initial_types=initial_type, options={type(model): {'zipmap': False}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mobility_model.onnx\", \"wb\") as f:\n",
    "    f.write(onx.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = rt.InferenceSession(\"mobility_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = sess.get_inputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = sess.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run([label_name], {input_name: X_test_onnx[:1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test_onnx[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
